{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for getting timeseries of related entities to the one defined using Graph traversal on Wikidata IDs to get related entities and then using news-signals in order to get the news volume timeseries of the related entities.\n",
    "\n",
    "Edit the below cell, \n",
    "\n",
    "**entity** = The surface entity, the root of the graph traversal\n",
    "\n",
    "**depth** = Depth of the graph search of related entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity = \"Elon Musk\"\n",
    "depth = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python library imports for this notebook, incase you want to run a particular cell block separetely (provided it doesnt have past variable dependencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we perform the graph traversal, a simple BFS approach, we get back the related entity IDs.\n",
    "\n",
    "The SPARQL **query** is inside the function, feel free to modify it according to needs. \n",
    "\n",
    "It is currently set to get entities of these types:\n",
    "\n",
    "**Humans (Q5)**\n",
    "\n",
    "**Organizations (Q43229)**\n",
    "\n",
    "**Companies (Q4830453)**\n",
    "\n",
    "**Products (Q2424752)**\n",
    "\n",
    "**Brands (Q431289)**\n",
    "\n",
    "**Publications (Q732577)**\n",
    "\n",
    "**Films (Q11424)**\n",
    "\n",
    "**Books (Q571)**\n",
    "\n",
    "You can build your query here [Query Builder](https://query.wikidata.org/querybuilder/?uselang=en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikidata ID for 'Elon Musk': Q317521\n",
      "Related entities (depth 1): ['Q202973', 'Q122374852', 'Q30686532', 'Q28874479', 'Q18325434', 'Q604444', 'Q112957545', 'Q105538968', 'Q111363577', 'Q918', 'Q111167889', 'Q1329269', 'Q109731214', 'Q46845259', 'Q101674980', 'Q7974160', 'Q30', 'Q122374820', 'Q193701', 'Q104721242', 'Q93418989', 'Q105424537', 'Q97572429', 'Q122374054', 'Q478214', 'Q258', 'Q48817614', 'Q29043471', 'Q269309', 'Q16', 'Q112626243', 'Q101675234', 'Q14590866', 'Q123885', 'Q131158869', 'Q112626244', 'Q67311526', 'Q1420038', 'Q1989', 'Q112626245', 'Q229166', 'Q6708744', 'Q95724881', 'Q10341331', 'Q7242167', 'Q104721244', 'Q41506', 'Q28534056', 'Q210893', 'Q21708200', 'Q49117', 'Q6318376', 'Q483959', 'Q101879184', 'Q671782', 'Q6409751', 'Q35723119', 'Q209896', 'Q1860', 'Q3926', 'Q117970', 'Q104101703', 'Q7555824', 'Q101675036', 'Q111204042', 'Q1427829', 'Q24007468', 'Q7827568', 'Q6173448']\n"
     ]
    }
   ],
   "source": [
    "def get_wikidata_id(entity_name):\n",
    "    \"\"\"\n",
    "    Given an entity name returns its Wikidata ID.\n",
    "    Uses Wikidata's wbsearchentities API.\n",
    "    \"\"\"\n",
    "    url = \"https://www.wikidata.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"wbsearchentities\",\n",
    "        \"format\": \"json\",\n",
    "        \"language\": \"en\",\n",
    "        \"search\": entity_name\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    \n",
    "    if \"search\" in data and data[\"search\"]:\n",
    "        # Return the first match\n",
    "        return data[\"search\"][0][\"id\"]\n",
    "    else:\n",
    "        print(f\"No Wikidata ID found for {entity_name}.\")\n",
    "        return None\n",
    "\n",
    "def get_related_entities(wikidata_id, depth=1):\n",
    "    \"\"\"\n",
    "    Given a Wikidata ID, performs a graph traversal to find related entities up to a specified depth.\n",
    "    \"Related\" means any entity connected via outgoing edges.\n",
    "    \n",
    "    Parameters:\n",
    "    - wikidata_id: The starting Wikidata ID (e.g., 'Q937' for Albert Einstein).\n",
    "    - depth: The number of hops (levels) to traverse.\n",
    "    \n",
    "    Returns:\n",
    "    - A list of Wikidata IDs that are related to the starting entity.\n",
    "    \n",
    "    Note: This implementation uses iterative breadth-first search (BFS) and can produce many queries.\n",
    "    \"\"\"\n",
    "    endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"GraphTraversalBot/1.0 (your_email@example.com) Python/requests\"\n",
    "    }\n",
    "    \n",
    "    visited = set()\n",
    "    current_level = {wikidata_id}\n",
    "    all_related = set()\n",
    "    \n",
    "    for d in range(depth):\n",
    "        next_level = set()\n",
    "        for item in current_level:\n",
    "            # This SPARQL query finds entities related to the current item (only outgoing edges).\n",
    "            query = f\"\"\"\n",
    "            SELECT ?related ?relatedLabel WHERE {{\n",
    "                wd:{item} ?prop ?related .\n",
    "                FILTER(isIRI(?related))\n",
    "                FILTER EXISTS {{\n",
    "                    ?related wdt:P31/wdt:P279* ?type .\n",
    "                    VALUES ?type {{ wd:Q5 wd:Q43229 wd:Q4830453 wd:Q2424752 wd:Q431289 wd:Q732577 wd:Q11424 wd:Q571 }}\n",
    "                }}\n",
    "                SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\" }}\n",
    "                }}\n",
    "            \"\"\"\n",
    "\n",
    "            response = requests.get(endpoint_url, params={'query': query, 'format': 'json'}, headers=headers)\n",
    "            result = response.json()\n",
    "            \n",
    "            for binding in result[\"results\"][\"bindings\"]:\n",
    "                related_uri = binding[\"related\"][\"value\"]\n",
    "                # Extract the Wikidata ID from the URI (e.g., http://www.wikidata.org/entity/Q42 -> Q42)\n",
    "                if related_uri.startswith(\"http://www.wikidata.org/entity/\"):\n",
    "                    related_id = related_uri.split(\"/\")[-1]\n",
    "                    if related_id not in visited:\n",
    "                        next_level.add(related_id)\n",
    "                        all_related.add(related_id)\n",
    "            visited.add(item)\n",
    "        current_level = next_level\n",
    "        if not current_level:\n",
    "            break  # no further nodes to traverse\n",
    "    return list(all_related)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    wikidata_id = get_wikidata_id(entity)\n",
    "    if wikidata_id:\n",
    "        print(f\"Wikidata ID for '{entity}': {wikidata_id}\")\n",
    "        # Change depth as needed; be cautious with high numbers!\n",
    "        related_entities = get_related_entities(wikidata_id, depth)\n",
    "        print(f\"Related entities (depth {depth}): {related_entities}\")\n",
    "    else:\n",
    "        print(\"Entity conversion failed. Check the input name.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the related entities IDs from wikidata, we can convert them back to human-readable format for our use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q202973 -> Kingston\n",
      "Q122374852 -> Azure Musk\n",
      "Q30686532 -> X.com\n",
      "Q28874479 -> The Boring Company\n",
      "Q18325434 -> Template:Elon Musk\n",
      "Q604444 -> University of Pretoria\n",
      "Q112957545 -> Shivon Zilis\n",
      "Q105538968 -> Alexandra Musk\n",
      "Q111363577 -> Asha Rose Musk\n",
      "Q918 -> X\n",
      "Q111167889 -> Musk family\n",
      "Q1329269 -> The Wharton School\n",
      "Q109731214 -> Nevada Musk\n",
      "Q46845259 -> Elon Musk's Tesla Roadster\n",
      "Q101674980 -> Unknown\n",
      "Q7974160 -> Waterkloof House Preparatory School\n",
      "Q30 -> United States of America\n",
      "Q122374820 -> Strider Musk\n",
      "Q193701 -> SpaceX\n",
      "Q104721242 -> Jana Bezuidenhout\n",
      "Q93418989 -> X Æ A-Ⅻ Musk\n",
      "Q105424537 -> Smith School of Business\n",
      "Q97572429 -> Kai Musk\n",
      "Q122374054 -> Tau Musk\n",
      "Q478214 -> Tesla, Inc.\n",
      "Q258 -> South Africa\n",
      "Q48817614 -> Bryanston High School\n",
      "Q29043471 -> Neuralink\n",
      "Q269309 -> Talulah Riley\n",
      "Q16 -> Canada\n",
      "Q112626243 -> X Holdings I, Inc.\n",
      "Q101675234 -> Damian Musk\n",
      "Q14590866 -> Category:Elon Musk\n",
      "Q123885 -> Royal Society\n",
      "Q131158869 -> Department of Government Efficiency\n",
      "Q112626244 -> X Holdings II, Inc.\n",
      "Q67311526 -> Obálky knih\n",
      "Q1420038 -> Queen's University\n",
      "Q1989 -> Saskatchewan\n",
      "Q112626245 -> X Holdings III, LLC\n",
      "Q229166 -> Amber Heard\n",
      "Q6708744 -> Lyndon Rive\n",
      "Q95724881 -> Griffin Musk\n",
      "Q10341331 -> Order of Defence Merit\n",
      "Q7242167 -> Pretoria Boys High School\n",
      "Q104721244 -> Elliot Rush Musk\n",
      "Q41506 -> Stanford University\n",
      "Q28534056 -> Natasha Bassett\n",
      "Q210893 -> Tesla Roadster (first generation)\n",
      "Q21708200 -> OpenAI\n",
      "Q49117 -> University of Pennsylvania\n",
      "Q6318376 -> Justine Musk\n",
      "Q483959 -> PayPal\n",
      "Q101879184 -> John Elon Haldeman\n",
      "Q671782 -> The Planetary Society\n",
      "Q6409751 -> Kimbal Musk\n",
      "Q35723119 -> Errol Musk\n",
      "Q209896 -> honorary degree\n",
      "Q1860 -> English\n",
      "Q3926 -> Pretoria\n",
      "Q117970 -> Grimes\n",
      "Q104101703 -> Clubhouse\n",
      "Q7555824 -> SolarCity\n",
      "Q101675036 -> Saxon Musk\n",
      "Q111204042 -> Exa Musk\n",
      "Q1427829 -> Mars Society\n",
      "Q24007468 -> Maye Musk\n",
      "Q7827568 -> Tosca Musk\n",
      "Q6173448 -> Wikipedia:Vital articles/Level/4\n"
     ]
    }
   ],
   "source": [
    "def get_labels_for_ids(wikidata_ids, language='en'):\n",
    "    \"\"\"\n",
    "    Convert a list of Wikidata IDs (including composite IDs) to human-readable labels.\n",
    "    This function extracts the base ID (everything before the first hyphen), removes duplicates,\n",
    "    and then queries Wikidata's API.\n",
    "    \n",
    "    Parameters:\n",
    "      - wikidata_ids: List of strings, e.g. [\"Q317521\", \"Q317521-XXXX\", ...]\n",
    "      - language: Language code for labels (default is 'en').\n",
    "      \n",
    "    Returns:\n",
    "      A dictionary mapping base Wikidata IDs to their human-readable labels.\n",
    "    \"\"\"\n",
    "    if not wikidata_ids:\n",
    "        print(\"No Wikidata IDs provided for label lookup.\")\n",
    "        return {}\n",
    "\n",
    "    # Extract base IDs (before the first hyphen) and remove duplicates\n",
    "    base_ids = list(set(wid.split('-')[0] for wid in wikidata_ids))\n",
    "    #print(f\"Extracted unique base IDs: {base_ids}\")  # Debugging\n",
    "\n",
    "    url = \"https://www.wikidata.org/w/api.php\"\n",
    "    headers = {\"User-Agent\": \"GraphTraversalBot/1.0\"}\n",
    "    labels = {}\n",
    "\n",
    "    MAX_IDS = 50  # Process in batches to avoid API limits\n",
    "\n",
    "    for i in range(0, len(base_ids), MAX_IDS):\n",
    "        batch_ids = base_ids[i:i + MAX_IDS]\n",
    "        ids_param = \"|\".join(batch_ids)\n",
    "\n",
    "        params = {\n",
    "            \"action\": \"wbgetentities\",\n",
    "            \"ids\": ids_param,\n",
    "            \"format\": \"json\",\n",
    "            \"props\": \"labels\",\n",
    "            \"languages\": language\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, params=params, headers=headers)\n",
    "        try:\n",
    "            data = response.json()\n",
    "            #print(\"API Response:\", data)  # Debugging\n",
    "\n",
    "            if \"error\" in data:\n",
    "                print(f\"Error fetching labels: {data['error']}\")\n",
    "                continue\n",
    "\n",
    "            for entity_id, entity_info in data.get(\"entities\", {}).items():\n",
    "                label = entity_info.get(\"labels\", {}).get(language, {}).get(\"value\", \"Unknown\")\n",
    "                labels[entity_id] = label\n",
    "\n",
    "        except requests.exceptions.JSONDecodeError:\n",
    "            print(f\"Failed to decode JSON response. Raw response: {response.text}\")\n",
    "            continue\n",
    "\n",
    "    return labels\n",
    "\n",
    "labels_dict = get_labels_for_ids(related_entities)\n",
    "\n",
    "for base_id, label in labels_dict.items():\n",
    "    print(f\"{base_id} -> {label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets save the results to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to labels_dict.csv\n"
     ]
    }
   ],
   "source": [
    "filename = 'labels_dict.csv'\n",
    "\n",
    "with open(filename, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Wikidata ID', 'Label'])\n",
    "    for key, value in labels_dict.items():\n",
    "        writer.writerow([key, value])\n",
    "\n",
    "print(f\"Data has been written to {filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "news-signals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
