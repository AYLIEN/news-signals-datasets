{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is an experiment for performing TABSA on news data. Using Named Entity Recognition (NER) from Spacy to automatically locate entities to perform TABSA (Targeted Aspect Based Sentiment Analysis on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import news_signals\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we generate the financial dataset using yfinance. We group the closing prices by the *window_size* parameter and apply +1, -1 classification if the price has fluctuated by 3% up or down respectively, otherwise put the classification at 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = \"TSLA\"            # Change ticker if needed\n",
    "start_date = \"2023-01-01\"    # Start date for historical data\n",
    "end_date = \"2023-12-31\"      # End date for historical data\n",
    "window_size = 3            # 3-day rolling window\n",
    "\n",
    "# Download daily stock data\n",
    "data = yf.download(ticker, start=start_date, end=end_date)\n",
    "data.index = pd.to_datetime(data.index)\n",
    "\n",
    "def classify_window(window):\n",
    "    \"\"\"\n",
    "        +1 if cumulative return > %change and > volatility  (upward trend)\n",
    "        -1 if cumulative return < -%change and < -volatility (downward trend)\n",
    "         0 otherwise (neutral)\n",
    "    \"\"\"\n",
    "    first_open = float(window['Open'].iloc[0])\n",
    "    last_close = float(window['Close'].iloc[-1])\n",
    "    cumulative_return = (last_close - first_open) / first_open\n",
    "    daily_returns = (window['Close'] - window['Open']) / window['Open']\n",
    "    volatility = float(daily_returns.std())\n",
    "    \n",
    "    if cumulative_return > 0.03 and cumulative_return > volatility:\n",
    "        return 1\n",
    "    elif cumulative_return < -0.03 and cumulative_return < -volatility:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "# Apply a rolling window to classify the trend for each period\n",
    "trend_results = []\n",
    "dates = []\n",
    "for i in range(window_size - 1, len(data)):\n",
    "    window = data.iloc[i - window_size + 1 : i + 1]\n",
    "    trend = classify_window(window)\n",
    "    trend_results.append(trend)\n",
    "    dates.append(data.index[i])\n",
    "\n",
    "# Create a DataFrame with the trend classifications (using the last day of each window as the index)\n",
    "rolling_trend_df = pd.DataFrame({'Trend': trend_results}, index=dates)\n",
    "print(rolling_trend_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use spacy in the extract_entities function to extract all entities from our news text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load spaCy's small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_entities(text):\n",
    "    \"\"\"\n",
    "    Extracts unique named entities using spaCy.\n",
    "    Returns a list of entities from the text.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    entities = list(set(ent.text for ent in doc.ents if ent.label_ in [\"ORG\", \"PERSON\", \"GPE\", \"PRODUCT\"]))  # Extract relevant entities\n",
    "    return entities\n",
    "\n",
    "# Load CSV\n",
    "news_df = pd.read_csv(\"entity_news_processed_azure_reduced.csv\") # adjust path to your news text data\n",
    "news_df[\"Processed_Article\"] = news_df[\"Processed_Article\"].fillna(\"\").astype(str)\n",
    "\n",
    "# Enable tqdm progress bar\n",
    "tqdm.pandas(desc=\"Extracting Named Entities\")\n",
    "\n",
    "# Apply NER to extract entities\n",
    "news_df[\"Entities\"] = news_df[\"Processed_Article\"].progress_apply(extract_entities)\n",
    "\n",
    "# Display first few rows to verify entity extraction\n",
    "#print(news_df[[\"published_at\", \"Entities\"]].head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some vanity code in order to parse NER entities and to sort them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "def robust_parse_entities(entities_str):\n",
    "    \"\"\"\n",
    "    Parse a string representation of a list (e.g., \"['A', 'B']\") into an actual list,\n",
    "    fixing some common quoting issues, and return a sorted list (alphabetically, case-insensitive).\n",
    "    If parsing fails, returns an empty list.\n",
    "    \"\"\"\n",
    "    if pd.isna(entities_str) or entities_str.strip() == \"\":\n",
    "        return []\n",
    "    try:\n",
    "        # Replace problematic double-double quotes with a single quote.\n",
    "        # For example:  \"\"Elon Musk's\"\" -> 'Elon Musk's'\n",
    "        s = entities_str.replace('\"\"', \"'\")\n",
    "        parsed = ast.literal_eval(s)\n",
    "        if isinstance(parsed, list):\n",
    "            # Filter to keep only strings and sort them alphabetically (case-insensitive)\n",
    "            sorted_list = sorted([item for item in parsed if isinstance(item, str)], key=lambda x: x.lower())\n",
    "            return sorted_list\n",
    "        else:\n",
    "            return parsed\n",
    "    except Exception as e:\n",
    "        print(\"Error parsing:\", entities_str, e)\n",
    "        return []\n",
    "\n",
    "# Load your NER output CSV\n",
    "df = pd.read_csv(\"ner_output.csv\")\n",
    "\n",
    "# Process the Entities column\n",
    "df[\"Entities\"] = df[\"Entities\"].apply(robust_parse_entities)\n",
    "\n",
    "# Remove the published_at column if present\n",
    "#if \"published_at\" in df.columns:\n",
    "#    df = df.drop(columns=[\"published_at\"])\n",
    "\n",
    "# Save the results to a new CSV file for manual pruning\n",
    "df.to_csv(\"ner_output.csv\", index=False)\n",
    "print(\"Sorted entities CSV saved to 'ner_output_sorted.csv'.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we finally perform TABSA on our data. We use the pyabsa library (*pip install pyabsa* if import fails). Feel free to change the parameters in *apc_model*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyabsa import APCCheckpointManager\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "# Suppress verbose logging\n",
    "logging.getLogger(\"pyabsa\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"transformers\").setLevel(logging.CRITICAL)\n",
    "\n",
    "# Load the ABSA model (with print_result and verbose turned off)\n",
    "apc_model = APCCheckpointManager.get_sentiment_classifier(\n",
    "    checkpoint=\"English\",\n",
    "    dataset=\"None\",\n",
    "    print_result=False,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "def targeted_aspect_sentiment(text, entities):\n",
    "    \"\"\"\n",
    "    Runs targeted ABSA on a given text using pruned entities as aspects.\n",
    "    Returns a dictionary mapping each entity to its sentiment score.\n",
    "    \n",
    "    Note: This function averages the sentiment for each entity **within the given text**.\n",
    "    That is, if an entity is mentioned multiple times in the article, it sums all\n",
    "    scores and then divides by the number of mentions.\n",
    "    \"\"\"\n",
    "    # Initialize scores and counts for the provided entities\n",
    "    aspect_scores = {entity: 0.0 for entity in entities}\n",
    "    counts = {entity: 0 for entity in entities}\n",
    "\n",
    "    if not entities or not isinstance(text, str) or not text.strip():\n",
    "        return aspect_scores\n",
    "\n",
    "    try:\n",
    "        results = apc_model.predict(text, print_result=False)\n",
    "\n",
    "        if isinstance(results, dict):\n",
    "            results = [results]\n",
    "        if not isinstance(results, list):\n",
    "            return aspect_scores\n",
    "\n",
    "        for result in results:\n",
    "            if isinstance(result, dict):\n",
    "                aspect_texts = result.get(\"aspect\", [])\n",
    "                sentiments = result.get(\"sentiment\", [])\n",
    "\n",
    "                if not isinstance(aspect_texts, list):\n",
    "                    aspect_texts = [aspect_texts]\n",
    "                if not isinstance(sentiments, list):\n",
    "                    sentiments = [sentiments]\n",
    "\n",
    "                for aspect_text, sentiment in zip(aspect_texts, sentiments):\n",
    "                    aspect_text = aspect_text.lower()\n",
    "                    sentiment = sentiment.lower()\n",
    "                    score = {\"positive\": 1, \"negative\": -1, \"neutral\": 0}.get(sentiment, 0)\n",
    "\n",
    "                    for entity in entities:\n",
    "                        if entity.lower() in aspect_text:\n",
    "                            aspect_scores[entity] += score\n",
    "                            counts[entity] += 1\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        # Average sentiment scores for each entity mentioned multiple times\n",
    "        for entity in entities:\n",
    "            if counts[entity] > 0:\n",
    "                aspect_scores[entity] /= counts[entity]\n",
    "\n",
    "    except RuntimeError:\n",
    "        return aspect_scores\n",
    "\n",
    "    return aspect_scores\n",
    "\n",
    "# Enable progress bar for DataFrame apply\n",
    "tqdm.pandas(desc=\"Running TABSA on Pruned Entities\")\n",
    "\n",
    "# Load your pruned entities CSV (assumed to have columns: published_at, Pruned_Entities)\n",
    "pruned_df = pd.read_csv(\"ner_output.csv\")\n",
    "# Convert the pruned entity string to a Python list if needed.\n",
    "# (Assumes you saved the list as a string representation; you might need to adjust this)\n",
    "pruned_df[\"Pruned_Entities\"] = pruned_df[\"Entities\"].apply(eval)\n",
    "\n",
    "# Merge the pruned entities back into your main news_df\n",
    "# Here, we assume that published_at is unique or that you can merge on it.\n",
    "# Otherwise, you might want to merge on an article ID.\n",
    "news_df = news_df.drop(columns=[\"Entities\"])  # Remove old entities if needed\n",
    "news_df = news_df.merge(pruned_df[[\"published_at\", \"Pruned_Entities\"]], on=\"published_at\", how=\"left\")\n",
    "\n",
    "# If no pruned entities found for an article, default to an empty list\n",
    "news_df[\"Pruned_Entities\"] = news_df[\"Pruned_Entities\"].apply(lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "# Run TABSA on each article using the pruned entity list\n",
    "news_df[\"Entity_Sentiments_TABSA\"] = news_df.progress_apply(\n",
    "    lambda row: targeted_aspect_sentiment(row[\"Processed_Article\"], row[\"Pruned_Entities\"]), axis=1\n",
    ")\n",
    "\n",
    "print(\"Completed running TABSA\")\n",
    "#print(news_df[[\"published_at\", \"Pruned_Entities\", \"Entity_Sentiments_TABSA\"]].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally,  after getting our results from TABSA, we adjust it to our financial data and concatenate it to the rolling window period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ensure `published_at` is a datetime column and remove timezone info\n",
    "news_df[\"published_at\"] = pd.to_datetime(news_df[\"published_at\"]).dt.tz_localize(None)\n",
    "\n",
    "# Reset index and sort financial data\n",
    "rolling_trend_df_reset = rolling_trend_df.reset_index().rename(columns={'index': 'Date'})\n",
    "rolling_trend_df_reset[\"Date\"] = pd.to_datetime(rolling_trend_df_reset[\"Date\"]).dt.tz_localize(None)  # Ensure it's timezone-naive\n",
    "rolling_trend_df_reset = rolling_trend_df_reset.sort_values(\"Date\")\n",
    "\n",
    "FINANCE_START_DATE = rolling_trend_df_reset[\"Date\"].min()\n",
    "ATTACHED_ASPECT_FEATURES = []\n",
    "\n",
    "prev_date = FINANCE_START_DATE\n",
    "\n",
    "# Ensure unique aspects exist to pre-fill default values\n",
    "all_unique_aspects = set()\n",
    "\n",
    "# Collect all unique aspects that appear in any row\n",
    "for entity_dict in news_df[\"Entity_Sentiments_TABSA\"].dropna():\n",
    "    if isinstance(entity_dict, dict):\n",
    "        all_unique_aspects.update(entity_dict.keys())\n",
    "\n",
    "# Convert to a sorted list for consistent column ordering\n",
    "all_unique_aspects = sorted(list(all_unique_aspects))\n",
    "\n",
    "# Process each time window\n",
    "for current_date in rolling_trend_df_reset[\"Date\"]:\n",
    "    # Select news articles published between prev_date and current_date\n",
    "    mask = (news_df[\"published_at\"] >= prev_date) & (news_df[\"published_at\"] < current_date)\n",
    "    window_news = news_df[mask]\n",
    "\n",
    "    if not window_news.empty:\n",
    "        # Extract only numerical aspect sentiment columns\n",
    "        aspect_sentiment_columns = window_news[\"Entity_Sentiments_TABSA\"].dropna().apply(pd.Series)\n",
    "\n",
    "        # Compute the mean sentiment scores for each entity\n",
    "        avg_scores = aspect_sentiment_columns.mean().to_dict()\n",
    "\n",
    "        # Ensure all aspects exist in the output (even if missing in this window)\n",
    "        avg_scores = {aspect: avg_scores.get(aspect, 0.0) for aspect in all_unique_aspects}\n",
    "    else:\n",
    "        # If no news articles exist in the window, fill with zeros for all aspects\n",
    "        avg_scores = {aspect: 0.0 for aspect in all_unique_aspects}\n",
    "\n",
    "    ATTACHED_ASPECT_FEATURES.append(avg_scores)\n",
    "    prev_date = current_date\n",
    "\n",
    "# Convert the aspect sentiment scores into a DataFrame\n",
    "aspect_features_df = pd.DataFrame(ATTACHED_ASPECT_FEATURES)\n",
    "\n",
    "# Merge with financial data\n",
    "final_financial_df = pd.concat([rolling_trend_df_reset.reset_index(drop=True), aspect_features_df], axis=1)\n",
    "\n",
    "# Save to CSV\n",
    "OUTPUT_FILENAME = \"financial_data_with_tabsa_sentiments.csv\"\n",
    "final_financial_df.to_csv(OUTPUT_FILENAME, index=False)\n",
    "\n",
    "print(f\"Saved updated financial data with aspect sentiment features to {OUTPUT_FILENAME}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "news-signals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
