{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following from **raw_news_sentiment.ipynb**, Since news data usually has a lot of noise and unrelated content. We can leverage an LLM using the Azure Endpoint to preprocess the data for us.\n",
    "\n",
    "Put in your Azure key and endpoint details below\n",
    "Ensure the endpoint URL is for chat completions, e.g.\n",
    "\n",
    "    \"https://<your-resource-name>.openai.azure.com/openai/deployments/<deployment-name>/chat/completions?api-version=2024-08-01-preview\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import news_signals\n",
    "import datetime\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from openai import AzureOpenAI\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uses yfinance library to get financial data, then classifies the trend of the market trend movement by checking if the price has moved by a certain percentage over a certain time period.\n",
    "\n",
    "Currently it is set for TSLA stock (Tesla) for the year 2023, over a 3 day rolling window of 3% threshold, i.e it gets financial data for Tesla stock for the entire year of 2023, it splits it into 3 day rolling windows, and gives classification of *+1 for upward trend, 0 for neutral (within threshold), -1 for a downward trend*, so if in 3 days, the stock has moved up or down more than 3%, it gets a +1 or -1 classification respectively, otherwise it gets a neutral 0.\n",
    "\n",
    "\n",
    "Modify **ticker** to whichever stock you wish to get data for (e.g TSLA,AAPL,JPM). **start_date** and **end_date** to specify the time period of the overall financial data, **window_size** for the rolling window days and **percent_change** to change the decimal percentage value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = \"TSLA\"            # Change ticker if needed\n",
    "start_date = \"2023-01-01\"    # Start date for historical data\n",
    "end_date = \"2023-12-31\"      # End date for historical data\n",
    "window_size = 3            # 3-day rolling window\n",
    "\n",
    "# Download daily stock data\n",
    "data = yf.download(ticker, start=start_date, end=end_date)\n",
    "data.index = pd.to_datetime(data.index)\n",
    "\n",
    "def classify_window(window):\n",
    "    \"\"\"\n",
    "        +1 if cumulative return > %change and > volatility  (upward trend)\n",
    "        -1 if cumulative return < -%change and < -volatility (downward trend)\n",
    "         0 otherwise (neutral)\n",
    "    \"\"\"\n",
    "    first_open = float(window['Open'].iloc[0])\n",
    "    last_close = float(window['Close'].iloc[-1])\n",
    "    cumulative_return = (last_close - first_open) / first_open\n",
    "    daily_returns = (window['Close'] - window['Open']) / window['Open']\n",
    "    volatility = float(daily_returns.std())\n",
    "    \n",
    "    if cumulative_return > 0.03 and cumulative_return > volatility:\n",
    "        return 1\n",
    "    elif cumulative_return < -0.03 and cumulative_return < -volatility:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "# Apply a rolling window to classify the trend for each period\n",
    "trend_results = []\n",
    "dates = []\n",
    "for i in range(window_size - 1, len(data)):\n",
    "    window = data.iloc[i - window_size + 1 : i + 1]\n",
    "    trend = classify_window(window)\n",
    "    trend_results.append(trend)\n",
    "    dates.append(data.index[i])\n",
    "\n",
    "# Create a DataFrame with the trend classifications (using the last day of each window as the index)\n",
    "rolling_trend_df = pd.DataFrame({'Trend': trend_results}, index=dates)\n",
    "print(rolling_trend_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can run this cell to check distribution of financial trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class_distribution = rolling_trend_df['Trend'].value_counts(normalize=True) * 100\n",
    "print(\"Class Distribution (Percentage):\")\n",
    "print(class_distribution)\n",
    "\n",
    "# Plot the class distribution as percentages\n",
    "class_distribution.plot(kind='bar', color=['red', 'blue', 'green'])\n",
    "plt.title('Class Distribution of Trends (Percentage)')\n",
    "plt.xlabel('Trend')\n",
    "plt.ylabel('Percentage')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This calls the Azure endpoint, the prompt is given to preprocess the news to reduce irrelevant content and noise, feel free to modify the prompt as necessary. The code loads the file where the news was saved in **raw_news_sentiment**. \n",
    "\n",
    "It iterates over each article and processes it one at a time. \n",
    "\n",
    "Keep in mind that each news article is a separate query to the endpoint before running on big datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "\n",
    "# 1) Load your CSV file\n",
    "df = pd.read_csv(\"entity_news_paged.csv\")\n",
    "\n",
    "# 2) Define a function to call the Azure OpenAI API (GPT-4) with your filtering prompt\n",
    "def process_article_with_azure(title, body):\n",
    "    prompt = f\"\"\"\n",
    "I am providing you with a news article. Only extract content about or related to Tesla, Elon Musk, TSLA stocks or things that could affect them.\n",
    "Give me the relevant content with context without any changes to the relevant text. Do not add headings like \"Sure, here are the paragraphs.\" \n",
    "Simply output the paragraphs, each separated by a blank line (or keep their original spacing). If the news article does not contain any relevant information, then only output \"Nope\". \n",
    "Do not specify paragraph numbers or anything, simply output their content without any identifiers. \n",
    "**Important**: \n",
    "- Do not paraphrase, summarize, or change the text in any way. \n",
    "- Output exactly the extracted paragraphs, one after another, with no additional commentary\n",
    "\n",
    "below is the article\n",
    "\n",
    "Title: {title}\n",
    "\n",
    "Body: {body}\n",
    "    \"\"\".strip()\n",
    "    \n",
    "     # Get API key and endpoint from environment variables\n",
    "    api_key = os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "    endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"api-key\": api_key\n",
    "    }\n",
    "    \n",
    "    # For the Chat Completions endpoint, use the \"messages\" format\n",
    "    payload = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"max_tokens\": 500,\n",
    "        \"temperature\": 0,\n",
    "        \"top_p\": 1,\n",
    "        \"n\": 1\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(endpoint, headers=headers, json=payload, timeout=60)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        # Check for the output in data[\"choices\"][0][\"message\"]\n",
    "        message = data[\"choices\"][0][\"message\"]\n",
    "        # Try \"content\" key first, then \"text\"\n",
    "        result = message.get(\"content\")\n",
    "        if result is None:\n",
    "            result = message.get(\"text\", \"\")\n",
    "        return result.strip()\n",
    "    except requests.RequestException as e:\n",
    "        return f\"Error calling Azure OpenAI API: {e}\"\n",
    "    except (KeyError, IndexError) as e:\n",
    "        return f\"Unexpected response structure: {e}\"\n",
    "\n",
    "# 3) Create a new column to store Azure's processed text\n",
    "processed_texts = []\n",
    "\n",
    "# 4) Iterate over rows, process each article, and collect responses\n",
    "num_articles = len(df)\n",
    "for idx, row in df.iterrows():\n",
    "    title = str(row[\"title\"])\n",
    "    body = str(row[\"body\"])\n",
    "    processed = process_article_with_azure(title, body)\n",
    "    processed_texts.append(processed)\n",
    "    print(f\"Processed {idx+1} / {num_articles} articles so far.\")\n",
    "    print(\"Azure Response:\\n\", processed, \"\\n\")\n",
    "    \n",
    "# 5) Add the processed text as a new column in the DataFrame\n",
    "df[\"Processed_Article\"] = processed_texts\n",
    "\n",
    "# 6) Save the updated DataFrame to a new CSV file (retaining all original columns)\n",
    "output_filename = \"entity_news_processed_azure.csv\"\n",
    "df.to_csv(output_filename, index=False)\n",
    "print(f\"Saved processed articles to {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vanity code to drop unneeded columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the processed CSV file\n",
    "df = pd.read_csv(\"entity_news_processed_azure.csv\")\n",
    "\n",
    "# Drop the \"title\" and \"body\" columns\n",
    "df_reduced = df.drop(columns=[\"title\", \"body\"])\n",
    "\n",
    "# Save the reduced DataFrame to a new CSV file\n",
    "output_filename = \"entity_news_processed_azure_reduced.csv\"\n",
    "df_reduced.to_csv(output_filename, index=False)\n",
    "print(f\"Saved reduced CSV with only the processed article and other columns to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is code to create a csv file with the financial trends and the news articles from the rolling window (previous date inclusive, current date exclusive) attached to each trend value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the processed news CSV (which should have columns \"published_at\" and \"Processed_Article\")\n",
    "news_df = pd.read_csv(\"entity_news_processed_azure_reduced.csv\")\n",
    "news_df[\"published_at\"] = pd.to_datetime(news_df[\"published_at\"]).dt.tz_convert(None)\n",
    "\n",
    "# Assume you already have the financial data DataFrame (rolling_trend_df) from your previous code\n",
    "rolling_trend_df_reset = rolling_trend_df.reset_index().rename(columns={'index': 'Date'})\n",
    "rolling_trend_df_reset[\"Date\"] = pd.to_datetime(rolling_trend_df_reset[\"Date\"])\n",
    "rolling_trend_df_reset = rolling_trend_df_reset.sort_values(\"Date\")\n",
    "print(\"Financial data with trends:\")\n",
    "print(rolling_trend_df_reset)\n",
    "\n",
    "# Define the starting boundary for the news window (finance start date)\n",
    "finance_start_date = pd.to_datetime(\"2023-01-01\")\n",
    "attached_news = []  # List to store combined news for each financial window\n",
    "\n",
    "prev_date = finance_start_date\n",
    "\n",
    "# Iterate over each financial date and attach processed news articles published in the interval [prev_date, current_date)\n",
    "for current_date in rolling_trend_df_reset[\"Date\"]:\n",
    "    # Select news articles with published_at >= prev_date and < current_date\n",
    "    mask = (news_df[\"published_at\"] >= prev_date) & (news_df[\"published_at\"] < current_date)\n",
    "    window_news = news_df[mask]\n",
    "    \n",
    "    # Combine the Processed_Article texts from the filtered news, separated by a blank line\n",
    "    combined_text = \"\\n\\n\".join(str(x) for x in window_news[\"Processed_Article\"].fillna(\"\").tolist())\n",
    "    attached_news.append(combined_text)\n",
    "    \n",
    "    # Update prev_date to the current financial date for the next window\n",
    "    prev_date = current_date\n",
    "\n",
    "# Add the combined processed news as a new column in the financial DataFrame\n",
    "rolling_trend_df_reset[\"Processed_News\"] = attached_news\n",
    "\n",
    "# Optionally, if you want to remove the original title and body columns (if present), you can drop them:\n",
    "# rolling_trend_df_reset = rolling_trend_df_reset.drop(columns=[\"title\", \"body\"], errors=\"ignore\")\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "output_filename = \"trend_processed_news.csv\"\n",
    "rolling_trend_df_reset.to_csv(output_filename, index=False)\n",
    "print(f\"Saved updated financial data with attached processed news to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vanity code to clean the news stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file with attached processed news\n",
    "df = pd.read_csv(\"trend_processed_news.csv\")\n",
    "\n",
    "# Remove all occurrences of \"Nope\" from the \"Processed_News\" column\n",
    "df[\"Processed_News\"] = df[\"Processed_News\"].str.replace(\"Nope\", \"\", regex=False)\n",
    "df[\"Processed_News\"] = df[\"Processed_News\"].str.replace(\"_blank\", \"\", regex=False)\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "output_filename = \"trend_processed_news_cleaned.csv\"\n",
    "df.to_csv(output_filename, index=False)\n",
    "print(f\"Saved cleaned CSV to {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the tf-idf vectorizer.\n",
    "\n",
    "Parameter information:\n",
    "\n",
    "**stop_words** : Removes common English words that do not add much meaning to the text.\n",
    "\n",
    "**ngram_range** : Extracts word sequences of size 2 (bigrams) and 3 (trigrams).\n",
    "\n",
    "**max_features** : Limits the vocabulary to the top 500,000 most important words.\n",
    "\n",
    "**min_df** : Ignores words that appear in fewer than 5 documents.\n",
    "\n",
    "**max_df** : Ignores words that appear in more than 20% of the documents.\n",
    "\n",
    "For more information, Please visit [TF-IDF Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"trend_processed_news_cleaned.csv\")\n",
    "\n",
    "# Fill missing values in the 'News' column with an empty string\n",
    "news_text = df[\"Processed_News\"].fillna(\"\").astype(str)\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(2,3),\n",
    "    max_features=500000,\n",
    "    min_df=5,\n",
    "    max_df=0.2\n",
    ")\n",
    "\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(news_text)\n",
    "\n",
    "# Display the shape of the resulting TF-IDF matrix\n",
    "print(\"TF-IDF matrix shape:\", tfidf_matrix.shape)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"First 20 feature names:\", feature_names[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we finally run tf-idf and get our results using *Logistic Regression* (Feel free to use any other models). We also get output on most important n-gram features for each trend class.\n",
    "\n",
    "More information on [TF-IDF and Logistic Regresison](https://medium.com/@tejasdalvi927/sentiment-analysis-using-tf-idf-and-logisticregression-5ccc4f5c4f81)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"Trend\"]\n",
    "\n",
    "# Split data into training and test sets (80/20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_matrix, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train a Logistic Regression classifier\n",
    "clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Display the test set accuracy and classification report\n",
    "print(\"Test set accuracy: {:.2f}%\".format(accuracy_score(y_test, y_pred) * 100))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Display top features for each trend class\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for class_label in np.unique(y_train):\n",
    "    class_index = list(clf.classes_).index(class_label)\n",
    "    coef = clf.coef_[class_index]\n",
    "    top_indices = np.argsort(coef)[-10:][::-1]\n",
    "    print(f\"\\nTop features for trend class {class_label}:\")\n",
    "    for i in top_indices:\n",
    "        print(f\"{feature_names[i]}: {coef[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to visualize class probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the probabilities for each class\n",
    "y_proba = clf.predict_proba(X_test)\n",
    "\n",
    "# Plot the probabilities\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, class_label in enumerate(clf.classes_):\n",
    "    plt.plot(y_test.index, y_proba[:, i], label=f'Class {class_label} Probability')\n",
    "\n",
    "# Plot the actual classes\n",
    "plt.scatter(y_test.index, y_test, color='black', marker='x', label='Actual Class')\n",
    "\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Class Probabilities vs Actual Classes')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to display training set accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the training set\n",
    "y_train_pred = clf.predict(X_train)\n",
    "\n",
    "# Display the training set accuracy and classification report\n",
    "print(\"Training set accuracy: {:.2f}%\".format(accuracy_score(y_train, y_train_pred) * 100))\n",
    "print(\"\\nClassification Report (Training Set):\")\n",
    "print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot the confusion matrix using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=clf.classes_, yticklabels=clf.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "news-signals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
