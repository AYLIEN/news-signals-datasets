{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for getting timeseries of related entities to the one defined using Graph traversal on Wikidata IDs to get related entities\n",
    "\n",
    "Edit the below cell, \n",
    "\n",
    "**entity** = The surface entity, the root of the graph traversal\n",
    "\n",
    "**depth** = Depth of the graph search of related entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity = \"Elon Musk\"\n",
    "depth = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python library imports for this notebook, incase you want to run a particular cell block separetely (provided it doesnt have past variable dependencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we perform the graph traversal, a simple BFS approach, we get back the related entity IDs.\n",
    "\n",
    "The SPARQL **query** is inside the function, feel free to modify it according to needs. \n",
    "\n",
    "It is currently set to get entities of these types:\n",
    "\n",
    "**Humans (Q5)**\n",
    "\n",
    "**Organizations (Q43229)**\n",
    "\n",
    "**Companies (Q4830453)**\n",
    "\n",
    "**Products (Q2424752)**\n",
    "\n",
    "**Brands (Q431289)**\n",
    "\n",
    "**Publications (Q732577)**\n",
    "\n",
    "**Films (Q11424)**\n",
    "\n",
    "**Books (Q571)**\n",
    "\n",
    "You can build your query here [Query Builder](https://query.wikidata.org/querybuilder/?uselang=en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikidata_id(entity_name):\n",
    "    \"\"\"\n",
    "    Given an entity name returns its Wikidata ID.\n",
    "    Uses Wikidata's wbsearchentities API.\n",
    "    \"\"\"\n",
    "    url = \"https://www.wikidata.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"wbsearchentities\",\n",
    "        \"format\": \"json\",\n",
    "        \"language\": \"en\",\n",
    "        \"search\": entity_name\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    \n",
    "    if \"search\" in data and data[\"search\"]:\n",
    "        # Return the first match\n",
    "        return data[\"search\"][0][\"id\"]\n",
    "    else:\n",
    "        print(f\"No Wikidata ID found for {entity_name}.\")\n",
    "        return None\n",
    "\n",
    "def get_related_entities(wikidata_id, depth=1):\n",
    "    \"\"\"\n",
    "    Given a Wikidata ID, performs a graph traversal to find related entities up to a specified depth.\n",
    "    \"Related\" means any entity connected via outgoing edges.\n",
    "    \n",
    "    Parameters:\n",
    "    - wikidata_id: The starting Wikidata ID (e.g., 'Q937' for Albert Einstein).\n",
    "    - depth: The number of hops (levels) to traverse.\n",
    "    \n",
    "    Returns:\n",
    "    - A list of Wikidata IDs that are related to the starting entity.\n",
    "    \n",
    "    Note: This implementation uses iterative breadth-first search (BFS) and can produce many queries.\n",
    "    \"\"\"\n",
    "    endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"GraphTraversalBot/1.0 (your_email@example.com) Python/requests\"\n",
    "    }\n",
    "    \n",
    "    visited = set()\n",
    "    current_level = {wikidata_id}\n",
    "    all_related = set()\n",
    "    \n",
    "    for d in range(depth):\n",
    "        next_level = set()\n",
    "        for item in current_level:\n",
    "            # This SPARQL query finds entities related to the current item (only outgoing edges).\n",
    "            query = f\"\"\"\n",
    "            SELECT ?related ?relatedLabel WHERE {{\n",
    "                wd:{item} ?prop ?related .\n",
    "                FILTER(isIRI(?related))\n",
    "                FILTER EXISTS {{\n",
    "                    ?related wdt:P31/wdt:P279* ?type .\n",
    "                    VALUES ?type {{ wd:Q5 wd:Q43229 wd:Q4830453 wd:Q2424752 wd:Q431289 wd:Q732577 wd:Q11424 wd:Q571 }}\n",
    "                }}\n",
    "                SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\" }}\n",
    "                }}\n",
    "            \"\"\"\n",
    "\n",
    "            response = requests.get(endpoint_url, params={'query': query, 'format': 'json'}, headers=headers)\n",
    "            result = response.json()\n",
    "            \n",
    "            for binding in result[\"results\"][\"bindings\"]:\n",
    "                related_uri = binding[\"related\"][\"value\"]\n",
    "                # Extract the Wikidata ID from the URI (e.g., http://www.wikidata.org/entity/Q42 -> Q42)\n",
    "                if related_uri.startswith(\"http://www.wikidata.org/entity/\"):\n",
    "                    related_id = related_uri.split(\"/\")[-1]\n",
    "                    if related_id not in visited:\n",
    "                        next_level.add(related_id)\n",
    "                        all_related.add(related_id)\n",
    "            visited.add(item)\n",
    "        current_level = next_level\n",
    "        if not current_level:\n",
    "            break  # no further nodes to traverse\n",
    "    return list(all_related)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    wikidata_id = get_wikidata_id(entity)\n",
    "    if wikidata_id:\n",
    "        print(f\"Wikidata ID for '{entity}': {wikidata_id}\")\n",
    "        # Change depth as needed; be cautious with high numbers!\n",
    "        related_entities = get_related_entities(wikidata_id, depth)\n",
    "        print(f\"Related entities (depth {depth}): {related_entities}\")\n",
    "    else:\n",
    "        print(\"Entity conversion failed. Check the input name.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the related entities IDs from wikidata, we can convert them back to human-readable format for our use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_for_ids(wikidata_ids, language='en'):\n",
    "    \"\"\"\n",
    "    Convert a list of Wikidata IDs (including composite IDs) to human-readable labels.\n",
    "    This function extracts the base ID (everything before the first hyphen), removes duplicates,\n",
    "    and then queries Wikidata's API.\n",
    "    \n",
    "    Parameters:\n",
    "      - wikidata_ids: List of strings, e.g. [\"Q317521\", \"Q317521-XXXX\", ...]\n",
    "      - language: Language code for labels (default is 'en').\n",
    "      \n",
    "    Returns:\n",
    "      A dictionary mapping base Wikidata IDs to their human-readable labels.\n",
    "    \"\"\"\n",
    "    if not wikidata_ids:\n",
    "        print(\"No Wikidata IDs provided for label lookup.\")\n",
    "        return {}\n",
    "\n",
    "    # Extract base IDs (before the first hyphen) and remove duplicates\n",
    "    base_ids = list(set(wid.split('-')[0] for wid in wikidata_ids))\n",
    "    #print(f\"Extracted unique base IDs: {base_ids}\")  # Debugging\n",
    "\n",
    "    url = \"https://www.wikidata.org/w/api.php\"\n",
    "    headers = {\"User-Agent\": \"GraphTraversalBot/1.0\"}\n",
    "    labels = {}\n",
    "\n",
    "    MAX_IDS = 50  # Process in batches to avoid API limits\n",
    "\n",
    "    for i in range(0, len(base_ids), MAX_IDS):\n",
    "        batch_ids = base_ids[i:i + MAX_IDS]\n",
    "        ids_param = \"|\".join(batch_ids)\n",
    "\n",
    "        params = {\n",
    "            \"action\": \"wbgetentities\",\n",
    "            \"ids\": ids_param,\n",
    "            \"format\": \"json\",\n",
    "            \"props\": \"labels\",\n",
    "            \"languages\": language\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, params=params, headers=headers)\n",
    "        try:\n",
    "            data = response.json()\n",
    "            #print(\"API Response:\", data)  # Debugging\n",
    "\n",
    "            if \"error\" in data:\n",
    "                print(f\"Error fetching labels: {data['error']}\")\n",
    "                continue\n",
    "\n",
    "            for entity_id, entity_info in data.get(\"entities\", {}).items():\n",
    "                label = entity_info.get(\"labels\", {}).get(language, {}).get(\"value\", \"Unknown\")\n",
    "                labels[entity_id] = label\n",
    "\n",
    "        except requests.exceptions.JSONDecodeError:\n",
    "            print(f\"Failed to decode JSON response. Raw response: {response.text}\")\n",
    "            continue\n",
    "\n",
    "    return labels\n",
    "\n",
    "labels_dict = get_labels_for_ids(related_entities)\n",
    "\n",
    "for base_id, label in labels_dict.items():\n",
    "    print(f\"{base_id} -> {label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets save the results to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'labels_dict.csv'\n",
    "\n",
    "with open(filename, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Wikidata ID', 'Label'])\n",
    "    for key, value in labels_dict.items():\n",
    "        writer.writerow([key, value])\n",
    "\n",
    "print(f\"Data has been written to {filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "news-signals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
