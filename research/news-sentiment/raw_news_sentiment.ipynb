{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Getting News and Financial Data and Using TF-IDF to extract features\n",
    "\n",
    "This notebook is a part of experiments for integration of financial and news data. It fetches financial data, relevant news data and uses tf-idf to get n-grams from the news relevant to the financial trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert your news-api credentials below (needed for creating news dataset), if you have a dataset of your own, it is fine to skip this part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWSAPI_APP_KEY = \"\"\n",
    "NEWSAPI_APP_ID = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary imports, if you want to run a cell individually, run this cell and then run the selected cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import news_signals\n",
    "import datetime\n",
    "import requests\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uses yfinance library to get financial data, then classifies the trend of the market trend movement by checking if the price has moved by a certain percentage over a certain time period.\n",
    "\n",
    "Currently it is set for TSLA stock (Tesla) for the year 2023, over a 3 day rolling window of 5% threshold, i.e it gets financial data for Tesla stock for the entire year of 2023, it splits it into 3 day rolling windows, and gives classification of *+1 for upward trend, 0 for neutral (within threshold), -1 for a downward trend*, so if in 3 days, the stock has moved up or down more than 5%, it gets a +1 or -1 classification respectively, otherwise it gets a neutral 0.\n",
    "\n",
    "\n",
    "Modify **ticker** to whichever stock you wish to get data for (e.g TSLA,AAPL,JPM). **start_date** and **end_date** to specify the time period of the overall financial data, **window_size** for the rolling window days and **percent_change** to change the decimal percentage value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Parameters for the financial data\n",
    "ticker = \"TSLA\"            # Change ticker if needed\n",
    "start_date = \"2023-01-01\"    # Start date for historical data\n",
    "end_date = \"2023-12-31\"      # End date for historical data\n",
    "window_size = 3            # 3-day rolling window\n",
    "percent_change = 0.05       # 5% minimum price change\n",
    "\n",
    "# Download daily stock data\n",
    "data = yf.download(ticker, start=start_date, end=end_date)\n",
    "data.index = pd.to_datetime(data.index)\n",
    "\n",
    "def classify_window(window):\n",
    "    \"\"\"\n",
    "        +1 if cumulative return > %change and > volatility  (upward trend)\n",
    "        -1 if cumulative return < %change and < -volatility (downward trend)\n",
    "         0 otherwise (neutral)\n",
    "    \"\"\"\n",
    "    first_open = float(window['Open'].iloc[0])\n",
    "    last_close = float(window['Close'].iloc[-1])\n",
    "    cumulative_return = (last_close - first_open) / first_open\n",
    "    daily_returns = (window['Close'] - window['Open']) / window['Open']\n",
    "    volatility = float(daily_returns.std())\n",
    "    \n",
    "    if cumulative_return > percent_change and cumulative_return > volatility:\n",
    "        return 1\n",
    "    elif cumulative_return < -percent_change and cumulative_return < -volatility:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Apply a rolling window to classify the trend for each period\n",
    "trend_results = []\n",
    "dates = []\n",
    "for i in range(window_size - 1, len(data)):\n",
    "    window = data.iloc[i - window_size + 1 : i + 1]\n",
    "    trend = classify_window(window)\n",
    "    trend_results.append(trend)\n",
    "    dates.append(data.index[i])\n",
    "\n",
    "# Create a DataFrame with the trend classifications (using the last day of each window as the index)\n",
    "rolling_trend_df = pd.DataFrame({'Trend': trend_results}, index=dates)\n",
    "print(rolling_trend_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can run this cell to check distribution of financial trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class_distribution = rolling_trend_df['Trend'].value_counts(normalize=True) * 100\n",
    "print(\"Class Distribution (Percentage):\")\n",
    "print(class_distribution)\n",
    "\n",
    "# Plot the class distribution as percentages\n",
    "class_distribution.plot(kind='bar', color=['red', 'blue', 'green'])\n",
    "plt.title('Class Distribution of Trends (Percentage)')\n",
    "plt.xlabel('Trend')\n",
    "plt.ylabel('Percentage')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This calls the news api and builds a csv dataset of news.\n",
    "\n",
    "Params to modify:\n",
    "\n",
    "  **published_at.start** and **published_at.end** : Input the range of dates from which you want the news from\n",
    "\n",
    "  **language** : Set language of news\n",
    "\n",
    "  **entities** : Specify entities to search and their respective overall prominence in the news article\n",
    "\n",
    "  **source.rankings.alexa.rank.min** and **source.rankings.alexa.rank.min** : The news traffic rankings of sources of articles to retrieve from\n",
    "\n",
    "\n",
    "For more information, please visit [NewsAPI Documentation](https://docs.aylien.com/newsapi/v6/getting-started/#overview)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "HEADERS = {\n",
    "    'X-AYLIEN-NewsAPI-Application-ID': NEWSAPI_APP_ID,\n",
    "    'X-AYLIEN-NewsAPI-Application-Key': NEWSAPI_APP_KEY\n",
    "}\n",
    "\n",
    "params = {\n",
    "    \"published_at.start\": \"2023-01-01T00:00:00.000Z\",\n",
    "    \"published_at.end\": \"2023-12-31T00:00:00.000Z\",\n",
    "    \"language\": \"(en)\",\n",
    "    \"entities\": '{{surface_forms:(\"TSLA\" OR \"Tesla\" OR \"Elon Musk\") AND overall_prominence:>=0.7}}',\n",
    "    \"source.rankings.alexa.rank.min\": \"1\",\n",
    "    \"source.rankings.alexa.rank.max\": 7,\n",
    "    \"per_page\": 100,\n",
    "}\n",
    "\n",
    "news_data = []\n",
    "cursor = \"*\" \n",
    "while cursor:\n",
    "    if cursor != \"*\":\n",
    "        params[\"cursor\"] = cursor\n",
    "\n",
    "    response = requests.get(\"https://api.aylien.com/v6/news/stories\", params=params, headers=HEADERS)\n",
    "    result = response.json()\n",
    "\n",
    "    stories = result.get(\"stories\", [])\n",
    "    if not stories:\n",
    "        print(\"No more articles found. Stopping pagination.\")\n",
    "        break  \n",
    "    for s in stories:\n",
    "        news_data.append({\n",
    "            \"author\": s.get(\"author\", \"Unknown\"),\n",
    "            \"published_at\": s.get(\"published_at\", \"\"),\n",
    "            \"title\": s.get(\"title\", \"\"),\n",
    "            \"body\": s.get(\"body\", \"\"),\n",
    "            \"source\": s.get(\"source\", {}).get(\"name\", \"\"),\n",
    "            \"url\": s.get(\"links\", {}).get(\"permalink\", \"\")\n",
    "        })\n",
    "\n",
    "    print(f\"Retrieved {len(stories)} articles. Total so far: {len(news_data)}\")\n",
    "\n",
    "    cursor = result.get(\"next_page_cursor\")\n",
    "\n",
    "news_df = pd.DataFrame(news_data)\n",
    "news_csv_file = \"entity_news_paged.csv\"\n",
    "news_df.to_csv(news_csv_file, index=False)\n",
    "print(f\"News data saved to {news_csv_file} with {len(news_df)} articles\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is code to create a csv file with the financial trends and the news articles from the rolling window (previous date inclusive, current date exclusive) attached to each trend value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.read_csv(\"entity_news_paged.csv\")\n",
    "news_df[\"published_at\"] = pd.to_datetime(news_df[\"published_at\"]).dt.tz_convert(None)\n",
    "\n",
    "rolling_trend_df_reset = rolling_trend_df.reset_index()\n",
    "rolling_trend_df_reset.rename(columns={'index': 'Date'}, inplace=True)\n",
    "rolling_trend_df_reset[\"Date\"] = pd.to_datetime(rolling_trend_df_reset[\"Date\"])\n",
    "# Ensure the financial data is sorted by Date\n",
    "rolling_trend_df_reset = rolling_trend_df_reset.sort_values(\"Date\")\n",
    "\n",
    "# Define the starting boundary for the news window (use your finance start date)\n",
    "finance_start_date = pd.to_datetime(\"2023-01-01\")\n",
    "\n",
    "attached_news = []\n",
    "\n",
    "# Set the initial previous date for the window\n",
    "prev_date = finance_start_date\n",
    "\n",
    "# Iterate over each financial date and attach news articles published in the interval [prev_date, current_date)\n",
    "for current_date in rolling_trend_df_reset[\"Date\"]:\n",
    "    # Filter news articles: published on or after prev_date and before current_date\n",
    "    mask = (news_df[\"published_at\"] >= prev_date) & (news_df[\"published_at\"] < current_date)\n",
    "    window_news = news_df[mask]\n",
    "    \n",
    "    # Combine news articles from this window:\n",
    "    # Concatenate the title and body for each article, separated by a newline.\n",
    "    combined_text = \"\\n\\n\".join((window_news[\"title\"] + \"\\n\" + window_news[\"body\"]).tolist())\n",
    "    attached_news.append(combined_text)\n",
    "    \n",
    "    prev_date = current_date\n",
    "\n",
    "rolling_trend_df_reset[\"News\"] = attached_news\n",
    "csv_filename = \"trend_news.csv\"\n",
    "rolling_trend_df_reset.to_csv(csv_filename, index=False)\n",
    "print(rolling_trend_df_reset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the tf-idf vectorizer.\n",
    "\n",
    "Parameter information:\n",
    "\n",
    "**stop_words** : Removes common English words that do not add much meaning to the text.\n",
    "\n",
    "**ngram_range** : Extracts word sequences of size 2 (bigrams) and 3 (trigrams).\n",
    "\n",
    "**max_features** : Limits the vocabulary to the top 500,000 most important words.\n",
    "\n",
    "**min_df** : Ignores words that appear in fewer than 5 documents.\n",
    "\n",
    "**max_df** : Ignores words that appear in more than 20% of the documents.\n",
    "\n",
    "For more information, Please visit [TF-IDF Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"trend_news.csv\")\n",
    "\n",
    "# Fill missing values in the 'News' column with an empty string\n",
    "news_text = df[\"News\"].fillna(\"\").astype(str)\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(2,3),\n",
    "    max_features=500000,\n",
    "    min_df=5,\n",
    "    max_df=0.2\n",
    ")\n",
    "\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(news_text)\n",
    "\n",
    "# Display the shape of the resulting TF-IDF matrix\n",
    "print(\"TF-IDF matrix shape:\", tfidf_matrix.shape)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"First 20 feature names:\", feature_names[:50])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we finally run tf-idf and get our results using *Logistic Regression* (Feel free to use any other models). We also get output on most important n-gram features for each trend class.\n",
    "\n",
    "More information on [TF-IDF and Logistic Regresison](https://medium.com/@tejasdalvi927/sentiment-analysis-using-tf-idf-and-logisticregression-5ccc4f5c4f81)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"Trend\"]\n",
    "\n",
    "# Split data into training and test sets (80/20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_matrix, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train a Logistic Regression classifier\n",
    "clf = LogisticRegression(max_iter=5000, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Display the test set accuracy and classification report\n",
    "print(\"Test set accuracy: {:.2f}%\".format(accuracy_score(y_test, y_pred) * 100))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Display top features for each trend class\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for class_label in np.unique(y_train):\n",
    "    class_index = list(clf.classes_).index(class_label)\n",
    "    coef = clf.coef_[class_index]\n",
    "    top_indices = np.argsort(coef)[-10:][::-1]\n",
    "    print(f\"\\nTop features for trend class {class_label}:\")\n",
    "    for i in top_indices:\n",
    "        print(f\"{feature_names[i]}: {coef[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a Grid Search code in order to find best parameters for our TF-IDF model. You can modify the features in **param_grid** to change what combination of features are tested.\n",
    "\n",
    "For more information on [Grid Search](https://scikit-learn.org/stable/modules/grid_search.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        stop_words=\"english\",\n",
    "        ngram_range=(1, 2),\n",
    "        max_features=3000,\n",
    "        min_df=3,\n",
    "        max_df=0.9)),\n",
    "    ('clf', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "    'tfidf__max_features': [1000, 3000, 5000],\n",
    "    'tfidf__min_df': [2, 3, 5],\n",
    "    'tfidf__max_df': [0.8, 0.9, 1.0],\n",
    "    'clf__C': [0.01, 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(news_text, df[\"Trend\"])\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: {:.2f}%\".format(grid_search.best_score_ * 100))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "news-signals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
