{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  \u0000\u0000\u0000\u0001Bud1\u0000\u0000\u0010\u0000\u0000\u0000\u0000\u0000\u0010\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000...      0\n",
      "1  The Trump administration is shattering a grues...      0\n",
      "2                                                ...      0\n",
      "3  WASHINGTON (AP) — AstraZeneca reported Monday ...      0\n",
      "4  Donald Trump says the government should get a ...      0\n",
      "Loaded 17365 news articles.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "#Download the dataset manually from Kaggle and place in the same folder as this jupyter notebook\n",
    "data_dir = \"archive\"  \n",
    "\n",
    "# Define expected categories and labels\n",
    "expected_categories = {\n",
    "    \"Center Data\": 0,\n",
    "    \"Right Data\": 1,\n",
    "    \"Left Data\": 2\n",
    "}\n",
    "\n",
    "data = []\n",
    "\n",
    "# Loop through each category folder\n",
    "for category, label in expected_categories.items():\n",
    "    category_path = os.path.join(data_dir, category)\n",
    "\n",
    "    # Make sure the folder exists before processing\n",
    "    if not os.path.isdir(category_path):\n",
    "        print(f\"⚠ Skipping missing folder: {category}\")\n",
    "        continue\n",
    "\n",
    "    # Walk through all subdirectories and files\n",
    "    for root, _, files in os.walk(category_path):\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(root, filename)\n",
    "\n",
    "            try:\n",
    "                # Try reading in UTF-8 first\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                    text = file.read()\n",
    "            except UnicodeDecodeError:\n",
    "                try:\n",
    "                    # If UTF-8 fails, try ISO-8859-1 \n",
    "                    with open(file_path, \"r\", encoding=\"ISO-8859-1\") as file:\n",
    "                        text = file.read()\n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Error reading file: {file_path} | Skipping... Error: {e}\")\n",
    "                    continue  # Skip the problematic file\n",
    "\n",
    "            # Append text and label to dataset list\n",
    "            data.append({\"text\": text, \"label\": label})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df.head())\n",
    "print(f\"Loaded {len(df)} news articles.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0: 3997 articles\n",
      "Label 1: 5564 articles\n",
      "Label 2: 7804 articles\n",
      "Center: 3997 articles\n",
      "Right: 5564 articles\n",
      "Left: 7804 articles\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "label_counts = Counter(df[\"label\"])\n",
    "\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"Label {label}: {count} articles\")\n",
    "\n",
    "label_mapping = {0: \"Center\", 1: \"Right\", 2: \"Left\"}\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"{label_mapping[label]}: {count} articles\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    3997\n",
      "2    3997\n",
      "1    3997\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "target_size = min(label_counts.values())  # 3997 (Center articles)\n",
    "\n",
    "# Downsample Right & Left articles\n",
    "df_center = df[df[\"label\"] == 0] \n",
    "df_right = df[df[\"label\"] == 1].sample(target_size, random_state=42)\n",
    "df_left = df[df[\"label\"] == 2].sample(target_size, random_state=42)\n",
    "\n",
    "df_balanced = pd.concat([df_center, df_right, df_left]).sample(frac=1, random_state=42)  # Shuffle\n",
    "\n",
    "print(df_balanced[\"label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/news-signals/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', '__index_level_0__'],\n",
      "    num_rows: 11991\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_pandas(df_balanced)\n",
    "\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 11991/11991 [00:08<00:00, 1424.01 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 11991\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "print(tokenized_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 9592\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 1199\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 1200\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "valid_test_split = train_test_split[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_test_split[\"train\"],\n",
    "    \"validation\": valid_test_split[\"train\"],\n",
    "    \"test\": valid_test_split[\"test\"]\n",
    "})\n",
    "\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pickled model...\n",
      "Model is ready\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from transformers import RobertaForSequenceClassification, Trainer, TrainingArguments, pipeline, AutoTokenizer\n",
    "\n",
    "MODEL_PICKLE_FILE = \"bias_classifier.pkl\"\n",
    "MODEL_NAME = \"roberta-base\"\n",
    "\n",
    "def train_model():\n",
    "    global classifier, model, tokenizer  # Ensure model and tokenizer are accessible globally\n",
    "\n",
    "    print(\"Training the model...\")\n",
    "\n",
    "    model = RobertaForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)  # Define tokenizer here\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=500,\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"validation\"]\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "    classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "    with open(MODEL_PICKLE_FILE, \"wb\") as f:\n",
    "        pickle.dump(classifier, f)\n",
    "\n",
    "    print(\"Model trained and saved as:\", MODEL_PICKLE_FILE)\n",
    "\n",
    "# Load or Train Model\n",
    "if os.path.exists(MODEL_PICKLE_FILE):\n",
    "    print(\"Loading pickled model...\")\n",
    "    with open(MODEL_PICKLE_FILE, \"rb\") as f:\n",
    "        classifier = pickle.load(f)\n",
    "    model = classifier.model\n",
    "    tokenizer = classifier.tokenizer \n",
    "else:\n",
    "    train_model()\n",
    "\n",
    "print(\"Model is ready\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mevaluate(dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest set results:\u001b[39m\u001b[38;5;124m\"\u001b[39m, results)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "results = trainer.evaluate(dataset[\"test\"])\n",
    "print(\"Test set results:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: The new policy aims to support small businesses through tax incentives.\n",
      "Predicted Bias: Left (Confidence: 0.42)\n",
      "\n",
      "Text: Government overreach is destroying personal freedoms.\n",
      "Predicted Bias: Right (Confidence: 0.77)\n",
      "\n",
      "Text: A balanced approach to social programs is necessary.\n",
      "Predicted Bias: Left (Confidence: 0.48)\n",
      "\n",
      "Text: Tax cuts for the wealthy will improve economic growth, says administration.\n",
      "Predicted Bias: Left (Confidence: 0.62)\n",
      "\n",
      "Text: Protesters demand stronger action on climate change from the government.\n",
      "Predicted Bias: Left (Confidence: 0.67)\n",
      "\n",
      "Text: Finally, taxpayer money is being redirected away from these left-wing indoctrination centers. It is encouraging to see that Trump is not just targeting Ivy League schools but extending this crackdown to universities across the board.\n",
      "Predicted Bias: Right (Confidence: 0.72)\n",
      "\n",
      "Text: The US plans to impose a 25% tariff on steel imports, but UK shares rose instead of falling in response to the news.\n",
      "Predicted Bias: Left (Confidence: 0.56)\n",
      "\n",
      "Text: After the ceasefire in Gaza, West Bank Palestinians face more Israeli barriers, traffic and misery\n",
      "Predicted Bias: Left (Confidence: 0.88)\n",
      "\n",
      "Text: Man charged over 'attempted murder of police officer' in Clydebank\n",
      "Predicted Bias: Left (Confidence: 0.53)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "label_mapping = {\n",
    "    \"LABEL_0\": \"Center\",\n",
    "    \"LABEL_1\": \"Right\",\n",
    "    \"LABEL_2\": \"Left\"\n",
    "}\n",
    "\n",
    "# Examples\n",
    "examples = [\n",
    "    \"The new policy aims to support small businesses through tax incentives.\",\n",
    "    \"Government overreach is destroying personal freedoms.\",\n",
    "    \"A balanced approach to social programs is necessary.\",\n",
    "    \"Tax cuts for the wealthy will improve economic growth, says administration.\",\n",
    "    \"Protesters demand stronger action on climate change from the government.\",\n",
    "    \"Finally, taxpayer money is being redirected away from these left-wing indoctrination centers. It is encouraging to see that Trump is not just targeting Ivy League schools but extending this crackdown to universities across the board.\",\n",
    "    \"The US plans to impose a 25% tariff on steel imports, but UK shares rose instead of falling in response to the news.\",\n",
    "    \"After the ceasefire in Gaza, West Bank Palestinians face more Israeli barriers, traffic and misery\",\n",
    "    \"Man charged over 'attempted murder of police officer' in Clydebank\"\n",
    "\n",
    "]\n",
    "\n",
    "for text in examples:\n",
    "    prediction = classifier(text)\n",
    "    \n",
    "    predicted_label = prediction[0][\"label\"]\n",
    "\n",
    "    readable_label = label_mapping[predicted_label]\n",
    "\n",
    "    confidence = prediction[0][\"score\"]\n",
    "\n",
    "    print(f\"Text: {text}\\nPredicted Bias: {readable_label} (Confidence: {confidence:.2f})\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "preds = trainer.predict(dataset[\"test\"])\n",
    "y_preds = np.argmax(preds.predictions, axis=1)  \n",
    "y_true = np.array(dataset[\"test\"][\"label\"])  \n",
    "accuracy = accuracy_score(y_true, y_preds)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(classification_report(y_true, y_preds, target_names=[\"Center\", \"Right\", \"Left\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import news_signals\n",
    "import json\n",
    "import requests\n",
    "\n",
    "NEWSAPI_APP_KEY = \"3fe25605f6f24e2fd93430a4552db8f1\"\n",
    "NEWSAPI_APP_ID = \"2e104416\"\n",
    "HEADERS = {\n",
    "    'X-AYLIEN-NewsAPI-Application-ID': NEWSAPI_APP_ID,\n",
    "    'X-AYLIEN-NewsAPI-Application-Key': NEWSAPI_APP_KEY\n",
    "}\n",
    "                               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"published_at\": \"[1DAY-NOW/DAY TO NOW]\",\n",
    "    \"language\": \"(en)\",\n",
    "    \"categories\": \"{{taxonomy:aylien AND id:(ay.appsci) AND score:>=0.65}}\",\n",
    "    \"source.rankings.alexa.rank.min\": \"1\",\n",
    "    \"source.rankings.alexa.rank.max\": 100,\n",
    "    \"per_page\": 100,\n",
    "}\n",
    "\n",
    "response = requests.get(\n",
    "    url='https://api.aylien.com/v6/news/stories',\n",
    "    params=params,\n",
    "    headers=HEADERS\n",
    ")\n",
    "result = json.loads(response.content)\n",
    "for s in result['stories']:\n",
    "    print(f\"Author:{s['author']}\")\n",
    "    print(f\"Published At: {s['published_at']}\")\n",
    "    print(f\"Title: {s['title']}\")\n",
    "    print(f\"Body: {s['body']}\") \n",
    "    print('-' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_articles = []\n",
    "\n",
    "for s in result[\"stories\"]:\n",
    "    article_body = s.get(\"body\", \"\") \n",
    "\n",
    "    if not article_body.strip():  \n",
    "        continue\n",
    "\n",
    "    prediction = classifier(article_body[:512])[0]  # Truncate to 512 tokens\n",
    "\n",
    "    predicted_label = label_mapping[prediction[\"label\"]]\n",
    "    confidence = round(prediction[\"score\"], 2)\n",
    "\n",
    "    classified_articles.append({\n",
    "        \"Published At\": s[\"published_at\"],\n",
    "        \"Title\": s[\"title\"],\n",
    "        \"Bias\": predicted_label,\n",
    "        \"Confidence\": confidence\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(classified_articles)\n",
    "print(df_results)\n",
    "\n",
    "df_results.to_csv(\"news_bias_results.csv\", index=False)\n",
    "print(\"Results saved to 'news_bias_results.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "news-signals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
